{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "yolo task=detect mode=train epochs=10 data=data.yaml model=yolov8m.pt imgsz=640 batch=2\n",
    "5hours\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dc8ec89d0b03021"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83252c05-e596-4e07-be54-3ef8fe8095da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b163411f-b686-4336-9f4d-1b585900d837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b3dd84-2117-447f-a148-d4f1424f5d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.18 üöÄ Python-3.8.17 torch-2.2.1 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4096MiB)\n",
      "Setup complete ‚úÖ (8 CPUs, 11.9 GB RAM, 426.8/446.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T02:51:56.744246900Z",
     "start_time": "2024-02-24T02:51:56.719332800Z"
    }
   },
   "id": "7cff5d561b4a41ae",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Para guardar una imagen con anotaciones resultantes de la detecci√≥n de objetos usando YOLO con Ultralytics y OpenCV, debes seguir unos pasos adicionales. El m√©todo plot() de los resultados de YOLO generalmente retorna una figura de Matplotlib, por lo tanto, no puedes directamente usar cv2.imshow o cv2.imwrite con esta figura. Necesitas primero convertir esta figura a un formato que OpenCV pueda manejar.\n",
    "\n",
    "Aqu√≠ te dejo un enfoque para hacerlo:\n",
    "\n",
    "Usa plot() para crear la figura con anotaciones.\n",
    "Guarda esta figura como una imagen temporal o en la memoria.\n",
    "Lee esta imagen utilizando OpenCV.\n",
    "Muestra y/o guarda la imagen con OpenCV.\n",
    "python\n",
    "Copy code\n",
    "_\n",
    "\n",
    "\n",
    "yolo task=detect    mode=train    model=yolov8n.yaml      args...\n",
    "          classify       predict        yolov8n-cls.yaml  args...\n",
    "          segment        val            yolov8n-seg.yaml  args...\n",
    "                         export         yolov8n.pt        format=onnx  args..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "888fa2b646085215"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 Egg, 60.8ms\n",
      "Speed: 7.0ms preprocess, 60.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pred'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m resultados \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(img, imgsz\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m640\u001B[39m, conf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# 'resultados.pred[0]' contiene las detecciones; cada detecci√≥n tiene el formato [x1, y1, x2, y2, confianza, clase]\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m detections \u001B[38;5;241m=\u001B[39m \u001B[43mresultados\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpred\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Itera sobre las detecciones para dibujar las anotaciones\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;241m*\u001B[39mxyxy, conf, \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m detections:\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Convierte las coordenadas a enteros\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'pred'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Inicializa el modelo YOLO\n",
    "model = YOLO('FOODZ.pt')\n",
    "\n",
    "# Lee la imagen\n",
    "img = cv2.imread('C:\\\\Users\\\\ijzep\\\\_IA_LAMBTON\\\\Capstone\\\\YOLO\\\\food_test\\\\f2.jpeg')\n",
    "\n",
    "# Realiza la predicci√≥n\n",
    "resultados = model.predict(img, imgsz=640, conf=0.5)\n",
    "\n",
    "# Obtiene la figura con anotaciones\n",
    "fig = resultados[0].plot()\n",
    "\n",
    "# Guarda la figura en un buffer (en memoria) para luego leerla con OpenCV\n",
    "fig.canvas.draw()  # Dibuja la figura\n",
    "img_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "img_data = img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "# Convierte el color de RGB (Matplotlib) a BGR (OpenCV)\n",
    "img_data = cv2.cvtColor(img_data, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Muestra la imagen con anotaciones\n",
    "cv2.imshow('Resultado', img_data)\n",
    "cv2.waitKey(0)  # Espera a que se presione una tecla para cerrar la ventana\n",
    "\n",
    "# Guarda la imagen con anotaciones\n",
    "cv2.imwrite('resultado_con_anotaciones.jpg', img_data)\n",
    "\n",
    "# Cierra las ventanas de OpenCV y limpia la figura de Matplotlib\n",
    "cv2.destroyAllWindows()\n",
    "plt.close(fig)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:01:32.446981900Z",
     "start_time": "2024-02-25T01:01:31.554738700Z"
    }
   },
   "id": "843ca3511b542cbb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 Egg, 63.7ms\n",
      "Speed: 5.2ms preprocess, 63.7ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m resultados \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(img, imgsz\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m640\u001B[39m, conf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.50\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Acceder a las detecciones en el objeto 'Results'\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m detecciones \u001B[38;5;241m=\u001B[39m \u001B[43mresultados\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mxyxy\u001B[49m[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# Las detecciones est√°n en el tensor .xyxy[0]\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Iterar sobre las detecciones para dibujar las anotaciones\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m deteccion \u001B[38;5;129;01min\u001B[39;00m detecciones:\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Extraer las coordenadas y la etiqueta de la clase de cada detecci√≥n\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'xyxy'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Inicializar el modelo YOLO\n",
    "model = YOLO(\"FOODZ.pt\")\n",
    "\n",
    "# Leer una imagen del disco\n",
    "img = cv2.imread(\"C:\\\\Users\\\\ijzep\\\\_IA_LAMBTON\\\\Capstone\\\\YOLO\\\\food_test\\\\f2.jpeg\")  # Aseg√∫rate de especificar la ruta correcta a tu imagen\n",
    "\n",
    "# Realizar la predicci√≥n en la imagen\n",
    "resultados = model.predict(img, imgsz=640, conf=0.50)\n",
    "\n",
    "# Acceder a las detecciones en el objeto 'Results'\n",
    "detecciones = resultados.xyxy[0]  # Las detecciones est√°n en el tensor .xyxy[0]\n",
    "\n",
    "# Iterar sobre las detecciones para dibujar las anotaciones\n",
    "for deteccion in detecciones:\n",
    "    # Extraer las coordenadas y la etiqueta de la clase de cada detecci√≥n\n",
    "    x1, y1, x2, y2, conf, cls = map(int, deteccion[:6])\n",
    "    \n",
    "    # Dibujar el cuadro delimitador en la imagen\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    \n",
    "    # Obtener el nombre de la clase\n",
    "    label = model.names[int(cls)]\n",
    "    \n",
    "    # Dibujar la etiqueta de la clase y la confianza en la imagen\n",
    "    cv2.putText(img, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "# Mostrar la imagen con anotaciones\n",
    "cv2.imshow(\"DETECCION Y SEGMENTACION\", img)\n",
    "cv2.waitKey(0)  # Espera a que se presione una tecla\n",
    "\n",
    "# Guardar la imagen con anotaciones\n",
    "cv2.imwrite(\"imagen_con_anotaciones.jpg\", img)\n",
    "\n",
    "# Cerrar todas las ventanas abiertas por OpenCV\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:13:59.768867300Z",
     "start_time": "2024-02-25T01:13:58.718037200Z"
    }
   },
   "id": "5042815c0f43f1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 Eggs, 60.7ms\n",
      "Speed: 5.5ms preprocess, 60.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 608)\n",
      "0: 640x608 1 Egg, 58.3ms\n",
      "Speed: 6.1ms preprocess, 58.3ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "0: 640x608 1 Egg, 58.1ms\n",
      "Speed: 6.0ms preprocess, 58.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "0: 640x608 1 bottle, 2 bowls, 4 oranges, 1 refrigerator, 54.7ms\n",
      "Speed: 4.0ms preprocess, 54.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "0: 640x608 1 bottle, 1 bowl, 1 orange, 1 refrigerator, 56.5ms\n",
      "Speed: 5.0ms preprocess, 56.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "0: 640x608 1 bottle, 53.6ms\n",
      "Speed: 4.0ms preprocess, 53.6ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "0: 640x608 1 bottle, 2 bowls, 3 oranges, 20.1ms\n",
      "Speed: 5.0ms preprocess, 20.1ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x608 1 bottle, 2 oranges, 11.7ms\n",
      "Speed: 4.5ms preprocess, 11.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x608 1 bottle, 12.5ms\n",
      "Speed: 3.0ms preprocess, 12.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 608)\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerias\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "# MODELO=\"FOODZ\"  #FOODZ.pt #yolov8m #yolov8n\n",
    "models=[\"FOODZ\", \"yolov8m\", \"yolov8n\"]\n",
    "# CONF=0.4\n",
    "confs=[0.4, 0.6, 0.8]\n",
    "\n",
    "for MODELO in models:\n",
    "    \n",
    "    # Leer nuestro modelo\n",
    "    model = YOLO(MODELO+'.pt')\n",
    "    for CONF in confs:\n",
    "        # Realizar VideoCaptura\n",
    "        # cap = cv2.VideoCapture(0)\n",
    "        img = cv2.imread('C:\\\\Users\\\\ijzep\\\\_IA_LAMBTON\\\\Capstone\\\\YOLO\\\\food_test\\\\f1.jpg')\n",
    "        \n",
    "        # Leemos resultados\n",
    "        resultados = model.predict(img, imgsz = 640, conf = CONF)\n",
    "        \n",
    "        # Mostramos resultados\n",
    "        anotaciones = resultados[0].plot()\n",
    "        \n",
    "        # Mostramos nuestros fotogramas\n",
    "        cv2.imshow(\"DETECCION Y SEGMENTACION\", anotaciones)\n",
    "        \n",
    "        # Guarda la imagen con anotaciones\n",
    "        cv2.imwrite(f'1imagen_con_anotaciones-{MODELO}-{CONF}.jpg', anotaciones)\n",
    "        \n",
    "        # cv2.waitKey(0)  # Espera a que se presione una tecla para cerrar la ventana\n",
    "        \n",
    "        \n",
    "        # cap.release()\n",
    "        cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:39:40.026628400Z",
     "start_time": "2024-02-25T01:39:37.309376200Z"
    }
   },
   "id": "b987b188f606e107",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "12b1e009ccbdf862"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
