{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Recommendation System\n",
    "Creating, Testing, and Tuning unsupervised learning methods to recommend relevant recipes based on ingredient and category preference\n",
    "\n",
    "Workflow:\n",
    "1. Load, aggregate, clean, and tokenize recipe text data\n",
    "    - Identify Food Recipe Specific stop words that might be useful to ignore (e.g. measurements, numbers)\n",
    "    - It's likely that recipe attributes will depend heavily on ingredients and cooking methods, not necessarily\n",
    "    - Think twice about assigning words as stopwords, they might end up being useful.\n",
    "    - You may want to lemmatize the data to reduce sparseness; check lemmatization process to see if it strips important words, foods, or ingredients. Also, remove punctuation: it won't help for keyword \n",
    "2. ~~Create Word Embeddings using Word2Vec or GloVe Models (Consider using pretrained word embeddings)~~\n",
    "    - Discuss in detail the reason to choose one over the other for this context\n",
    "    - Setup neural network locally and run remotely on google colab\n",
    "    - I want to try tfidf and GloVe model, because tfidf doesn't take into account the order of words, which is isn't such a problem with recipes - it's the ingredients and cooking techniques that matter more. However, GloVe word vectors may be able to produce new words outside of the corpora text when summarizing the documents\n",
    "3. Compare Topic Extraction Methods\n",
    "    - ~~LDA2Vec~~\n",
    "    - LDA\n",
    "    - NNMF\n",
    "4. Generate keywords using keyword summarization and textrank\n",
    "    - Create methodology selectively assigns generated categories (e.g. LSA/NNMF Score must be above certain score threshold)\n",
    "    - Define metrics that evaluate the validity, breadth, and descriptive value of the assigned categories\n",
    "    - Identify Food Recipe Specific stop words that passed that might be suitable in the filter.\n",
    "5. Create extra features useful for search result ranking\n",
    "    - ~~Difficulty (Time, Number of Ingredients, Servings (inverse relationship),~~\n",
    "    - Import Unsupervised Generated Categories\n",
    "    - Create ratings that calculate overall weight.\n",
    "6. Find similarity scoring methods that would work best in this context. Some variation of Cosine Similarity will work best\n",
    "7. Create algorithm that utilizes similarity to sort recipes based on user-inputted queries, and sort base on other features as well.\n",
    "\n",
    "-----\n",
    "\n",
    "Regarding the Available Recipe Images\n",
    "\n",
    "Around 70,000 recipes out of the 125,000 have corresponding images, so it's possible to utilize these images to improve the models or create seperate, supplementary model\n",
    "\n",
    "Ideas:\n",
    "- Training a neural network to identify/predict/generate categories of foods based on their images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "Although I can no longer find the direct download, the link and code for the scraper the original user used to collect the data set is [here](https://github.com/rtlee9/recipe-box). This user collected the title, ingredients, and instructions from recipes found on Allrecipes.com, epicurious.com, and foodnetwork.com.\n",
    "\n",
    "For this project, the data was directly downloaded and uploaded for the creation of my own model. All the code present are of my own creation or significantly modified from the Thinkful curriculum. No other software sources were used verbatim within this project."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\~~Putting it all together~~\n",
    "\n",
    "Reintialize the model with 'pseudo-optimized' parameters, more easily track flow of data, and toggle with parameters all in one place. The \"database\" will also be created so that user queries will return results in a speedy manner!\n",
    "\n",
    "Some pieces of code will be commented out with a triple ***\\''' '''*** to indicate that the code takes too long to run and should only be run when the kernel has been shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_tfidf = vectorizer.fit_transform(tokenized_text)'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import string\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "\n",
    "### Below is all the code necessary to clean the data into useable form for modeling.\n",
    "'''\n",
    "# Loading Data\n",
    "allrecipes_raw = pd.read_json('../__DATA__/recipes_raw/recipes_raw_nosource_ar.json')\n",
    "\n",
    "allrecipes = allrecipes_raw.copy().T.reset_index().drop(columns = ['index'])\n",
    "recipes = pd.concat([allrecipes, epicurious, foodnetwork]).reset_index(drop=True) # Concat does not reset indices\n",
    "\n",
    "# Cleaning\n",
    "null_recs = recipes.copy().drop(columns = 'picture_link').T.isna().any()\n",
    "rows_to_drop = recipes[null_recs].index\n",
    "recipes = recipes.drop(index = rows_to_drop).reset_index(drop = True)\n",
    "\n",
    "nc_ingred_index = [index for i, index in zip(recipes['ingredients'], recipes.index) if all(j.isdigit() or j in string.punctuation for j in i)]\n",
    "nc_title_index = [index for i, index in zip(recipes['title'], recipes.index) if all(j.isdigit() or j in string.punctuation for j in i)]\n",
    "nc_instr_index = [index for i, index in zip(recipes['instructions'], recipes.index) if all(j.isdigit() or j in string.punctuation for j in i)]\n",
    "\n",
    "index_list = [nc_ingred_index, nc_title_index, nc_instr_index]\n",
    "\n",
    "inds_to_drop = set(reduce(add, index_list))\n",
    "print(len(inds_to_drop))\n",
    "recipes = recipes.drop(index=inds_to_drop).reset_index(drop=True)\n",
    "recipes.shape\n",
    "\n",
    "empty_instr_ind = [index for i, index in zip(recipes['instructions'], recipes.index) if len(i) < 20]\n",
    "recipes = recipes.drop(index = empty_instr_ind).reset_index(drop=True)\n",
    "\n",
    "ingredients = []\n",
    "for ing_list in recipes['ingredients']:\n",
    "    clean_ings = [ing.replace('ADVERTISEMENT','').strip() for ing in ing_list]\n",
    "    if '' in clean_ings:\n",
    "        clean_ings.remove('')\n",
    "    ingredients.append(clean_ings)\n",
    "recipes['ingredients'] = ingredients\n",
    "\n",
    "recipes['ingredient_text'] = ['; '.join(ingredients) for ingredients in recipes['ingredients']]\n",
    "recipes['ingredient_text'].head()\n",
    "\n",
    "recipes['ingredient_count'] = [len(ingredients) for ingredients in recipes['ingredients']]\n",
    "\n",
    "all_text = recipes['title'] + ' ' + recipes['ingredient_text'] + ' ' + recipes['instructions']\n",
    "\n",
    "def clean_text(documents):\n",
    "    cleaned_text = []\n",
    "    for doc in documents:\n",
    "        doc = doc.translate(str.maketrans('', '', string.punctuation)) # Remove Punctuation\n",
    "        doc = re.sub(r'\\d+', '', doc) # Remove Digits\n",
    "        doc = doc.replace('\\n',' ') # Remove New Lines\n",
    "        doc = doc.strip() # Remove Leading White Space\n",
    "        doc = re.sub(' +', ' ', doc) # Remove multiple white spaces\n",
    "        cleaned_text.append(doc)\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_text = clean_text(all_text)\n",
    "\n",
    "# Testing Strategies and Code\n",
    "nlp = spacy.load('en')\n",
    "' '.join([token.lemma_ for token in nlp(cleaned_text[2]) if not token.is_stop])\n",
    "\n",
    "def text_tokenizer_mp(doc):\n",
    "    tok_doc = ' '.join([token.lemma_ for token in nlp(doc) if not token.is_stop])\n",
    "    return tok_doc\n",
    "\n",
    "# Parallelzing tokenizing process\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "tokenized_text = pool.map(text_tokenizer_mp, [doc for doc in cleaned_text])\n",
    "'''\n",
    "\n",
    "# Creating TF-IDF Matrices and recalling text dependencies\n",
    "\n",
    "'''import text_tokenized.csv here to'''\n",
    "\n",
    "# TF-IDF vectorizer instance\n",
    "'''vectorizer = TfidfVectorizer(lowercase = True,\n",
    "                            ngram_range = (1,1))'''\n",
    "\n",
    "'''text_tfidf = vectorizer.fit_transform(tokenized_text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set All Recommendation Model Parameters\n",
    "N_topics = 50             # Number of Topics to Extract from corpora\n",
    "N_top_docs = 200          # Number of top documents within each topic to extract keywords\n",
    "N_top_words = 25          # Number of keywords to extract from each topic\n",
    "N_docs_categorized = 2000 # Number of top documents within each topic to tag \n",
    "N_neighbor_window = 4     # Length of word-radius that defines the neighborhood for\n",
    "                          # each word in the TextRank adjacency table\n",
    "\n",
    "# Query Similarity Weights\n",
    "w_title = 0.2\n",
    "w_text = 0.3\n",
    "w_categories = 0.5\n",
    "w_array = np.array([w_title, w_text, w_categories])\n",
    "\n",
    "# Recipe Stopwords: for any high volume food recipe terminology that doesn't contribute\n",
    "# to the searchability of a recipe. This list must be manually created.\n",
    "recipe_stopwords = ['cup','cups','ingredient','ingredients','teaspoon','teaspoons','tablespoon',\n",
    "                   'tablespoons','C','F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Data Dependencies\n",
    "topic_transformed_matrix = text_nmf\n",
    "root_text_data = cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating  tags (keywords/categories) and assigning to corresponding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "#recipes['tag_list'] = [[] for i in repeat(None, recipes.shape[0])]\n",
    "\n",
    "def topic_docs_4kwsummary(topic_document_scores, root_text_data):\n",
    "    '''Gathers and formats the top recipes in each topic'''\n",
    "    text_index = pd.Series(topic_document_scores).sort_values(ascending = False)[:N_top_docs].index\n",
    "    text_4kwsummary = pd.Series(root_text_data)[text_index]\n",
    "    return text_4kwsummary\n",
    "\n",
    "def generate_filter_kws(text_list):\n",
    "    '''Filters out specific parts of speech and stop words from the list of potential keywords'''\n",
    "    parsed_texts = nlp(' '.join(text_list)) \n",
    "    kw_filts = set([str(word) for word in parsed_texts \n",
    "                if (word.pos_== ('NOUN' or 'ADJ' or 'VERB'))\n",
    "                and word.lemma_ not in recipe_stopwords])\n",
    "    return list(kw_filts), parsed_texts\n",
    "\n",
    "def generate_adjacency(kw_filts, parsed_texts):\n",
    "    '''Tabulates counts of neighbors in the neighborhood window for each unique word'''\n",
    "    adjacency = pd.DataFrame(columns=kw_filts, index=kw_filts, data = 0)\n",
    "    for i, word in enumerate(parsed_texts):\n",
    "        if any ([str(word) == item for item in kw_filts]):\n",
    "            end = min(len(parsed_texts), i+N_neighbor_window+1) # Neighborhood Window Utilized Here\n",
    "            nextwords = parsed_texts[i+1:end]\n",
    "            inset = [str(x) in kw_filts for x in nextwords]\n",
    "            neighbors = [str(nextwords[i]) for i in range(len(nextwords)) if inset[i]]\n",
    "            if neighbors:\n",
    "                adjacency.loc[str(word), neighbors] += 1\n",
    "    return adjacency\n",
    "                \n",
    "def generate_wordranks(adjacency):\n",
    "    '''Runs TextRank on adjacency table'''\n",
    "    nx_words = nx.from_numpy_matrix(adjacency.values)\n",
    "    ranks=nx.pagerank(nx_words, alpha=.85, tol=.00000001)\n",
    "    return ranks\n",
    "\n",
    "def generate_tag_list(ranks):\n",
    "    '''Uses TextRank ranks to return actual key words for each topic in rank order'''\n",
    "    rank_values = [i for i in ranks.values()]\n",
    "    ranked = pd.DataFrame(zip(rank_values, list(kw_filts))).sort_values(by=0,axis=0,ascending=False)\n",
    "    kw_list = ranked.iloc[:N_top_words,1].to_list()\n",
    "    return kw_list\n",
    "\n",
    "# Master Function utilizing all above functions\n",
    "def generate_tags(topic_document_scores, root_text_data):\n",
    "    text_4kwsummary = topic_docs_4kwsummary(topic_document_scores, root_text_data)\n",
    "    kw_filts, parsed_texts = generate_filter_kws(text_4kwsummary)\n",
    "    adjacency = generate_adjacency(kw_filts, parsed_texts)\n",
    "    ranks = generate_wordranks(adjacency)\n",
    "    kw_list = generate_tag_list(ranks)\n",
    "    return kw_list\n",
    "\n",
    "def generate_kw_index(topic_document_scores):\n",
    "    kw_index = pd.Series(topic_document_scores).sort_values(ascending = False)[:N_docs_categorized].index\n",
    "    return kw_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency(kw_filts, parsed_texts):\n",
    "    adjacency = pd.DataFrame(columns=kw_filts, index=kw_filts, data=0)\n",
    "    for i, word in enumerate(parsed_texts):\n",
    "        if any([str(word) == item for item in kw_filts]):\n",
    "            end = min(len(parsed_texts), i + 5)  # Window of four words\n",
    "            nextwords = parsed_texts[i + 1:end]\n",
    "            inset = [str(x) in kw_filts for x in nextwords]\n",
    "            neighbors = [str(nextwords[i]) for i in range(len(nextwords)) if inset[i]]\n",
    "            if neighbors:\n",
    "                for neighbor in neighbors:\n",
    "                    adjacency.loc[str(word), neighbor] += 1\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0 Checkpoint\n",
      "Topic #10 Checkpoint\n",
      "Topic #20 Checkpoint\n",
      "Topic #30 Checkpoint\n",
      "Topic #40 Checkpoint\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# # Generating Tags and distributing to relevant documents\n",
    "# for i in range(topic_transformed_matrix.shape[1]):\n",
    "#     scores = topic_transformed_matrix[:, i]\n",
    "#     topic_kws = generate_tags(scores, root_text_data)\n",
    "#     kw_index_4df = generate_kw_index(scores)\n",
    "#     \n",
    "#     # Remove duplicates from kw_index_4df\n",
    "#     kw_index_4df_unique = kw_index_4df.drop_duplicates()\n",
    "#     \n",
    "#     # Iterate over unique index values and update DataFrame\n",
    "#     for idx in kw_index_4df_unique:\n",
    "#         if idx in recipes.index:\n",
    "#             if 'tag_list' not in recipes.columns:\n",
    "#                 recipes['tag_list'] = ''  # Create the 'tag_list' column if it doesn't exist\n",
    "#             recipes.at[idx, 'tag_list'] += ', '.join(topic_kws)  # Convert list to string and concatenate\n",
    "#     \n",
    "#     if i % 10 == 0:\n",
    "#         print('Topic #{} Checkpoint'.format(i))\n",
    "# \n",
    "# print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the precious dataframe so that I never have to calculate that again.\n",
    "# recipes.to_csv('tagged_recipes_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0 Checkpoint\n"
     ]
    }
   ],
   "source": [
    "# # Generating Tags and distributing to relevant documents\n",
    "# for i in range(topic_transformed_matrix.shape[1]):\n",
    "#     scores = topic_transformed_matrix[:, i]\n",
    "#     topic_kws = generate_tags(scores, root_text_data)\n",
    "#     kw_index_4df = generate_kw_index(scores)\n",
    "#     \n",
    "#     # Iterate over unique index values and update DataFrame\n",
    "#     for idx in kw_index_4df:\n",
    "#         if idx in recipes.index:\n",
    "#             if 'tag_list' not in recipes.columns:\n",
    "#                 recipes['tag_list'] = ''  # Create the 'tag_list' column if it doesn't exist\n",
    "#             recipes.at[idx, 'tag_list'] = ', '.join([recipes.at[idx, 'tag_list']] + topic_kws)  # Concatenate strings\n",
    "#     \n",
    "#     if i % 10 == 0:\n",
    "#         print('Topic #{} Checkpoint'.format(i))\n",
    "# \n",
    "# print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.loc[:5,'tag_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating lists of tags into a string a collective of tags for each documents\n",
    "recipes['tags'] = [' '.join(tags) for tags in recipes['tag_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.loc[:5,'tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Algorithm\n",
    "The final product presented is a search algorithm that takes in a list of ingredients or categories, and uses the query to return relavant recipes that utilize those ingredients or are similarly related to other ingredients and those recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TF-IDF Matrices and recalling text dependencies\n",
    "\n",
    "'''import text_tokenized.csv here'''\n",
    "\n",
    "# TF-IDF vectorizer instance\n",
    "'''vectorizer = TfidfVectorizer(lowercase = True,\n",
    "                            ngram_range = (1,1))'''\n",
    "\n",
    "'''text_tfidf = vectorizer.fit_transform(tokenized_text)'''\n",
    "# title_tfidf = vectorizer.transform(recipes['title'])\n",
    "# text_tfidf    <== Variable with recipe ingredients and instructions\n",
    "# tags_tfidf = vectorizer.transform(recipes['tags'])\n",
    "# recipes   <== DataFrame; For indexing and printing recipes\n",
    "\n",
    "# Query Similarity Weights\n",
    "w_title = .2\n",
    "w_text = .3\n",
    "w_categories = .5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qweight_array(query_length, qw_array = [1]):\n",
    "    '''Returns descending weights for ranked query ingredients'''\n",
    "    if query_length > 1:\n",
    "        to_split = qw_array.pop()\n",
    "        split = to_split/2\n",
    "        qw_array.extend([split, split])\n",
    "        return qweight_array(query_length - 1, qw_array)\n",
    "    else:\n",
    "        return np.array(qw_array)\n",
    "\n",
    "def ranked_query(query):\n",
    "    '''Called if query ingredients are ranked in order of importance.\n",
    "    Weights and adds each ranked query ingredient vector.'''\n",
    "    query = [[q] for q in query]      # place words in seperate documents\n",
    "    q_vecs = [vectorizer.transform(q) for q in query] \n",
    "    qw_array = qweight_array(len(query),[1])\n",
    "    q_weighted_vecs = q_vecs * qw_array\n",
    "    q_final_vector = reduce(np.add,q_weighted_vecs)\n",
    "    return q_final_vector\n",
    "\n",
    "def overall_scores(query_vector):\n",
    "    '''Calculates Query Similarity Scores against recipe title, instructions, and keywords.\n",
    "    Then returns weighted averages of similarities for each recipe.'''\n",
    "    final_scores = title_tfidf*query_vector.T*w_title\n",
    "    final_scores += text_tfidf*query_vector.T*w_text\n",
    "    final_scores += tags_tfidf*query_vector.T*w_categories\n",
    "    return final_scores\n",
    "\n",
    "def print_recipes(index, query, recipe_range):\n",
    "    '''Prints recipes according to query similary ranks'''\n",
    "    print('Search Query: {}\\n'.format(query))\n",
    "    for i, index in enumerate(index, recipe_range[0]):\n",
    "        print('Recipe Rank: {}\\t'.format(i+1),recipes.loc[index, 'title'],'\\n')\n",
    "        print('Ingredients:\\n{}\\n '.format(recipes.loc[index, 'ingredient_text']))\n",
    "        print('Instructions:\\n{}\\n'.format(recipes.loc[index, 'instructions']))\n",
    "        \n",
    "def Search_Recipes(query, query_ranked=False, recipe_range=(0,3)):\n",
    "    '''Master Recipe Search Function'''\n",
    "    if query_ranked == True:\n",
    "        q_vector = ranked_query(query)\n",
    "    else:\n",
    "        q_vector = vectorizer.transform([' '.join(query)])\n",
    "    recipe_scores = overall_scores(q_vector)\n",
    "    sorted_index = pd.Series(recipe_scores.toarray().T[0]).sort_values(ascending = False)[recipe_range[0]:recipe_range[1]].index\n",
    "    return print_recipes(sorted_index, query, recipe_range)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['cinnamon', 'cream', 'banana']\n",
    "Search_Recipes(query, query_ranked=True, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Rank\n",
    "query = ['wine', 'cilantro','butter']\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### -- Conclusions and Model Outlook --\n",
    "\n",
    "Overall the Search_Recipes function works quite well. From experimenting with the weighting, it's clear to me that the original text of the recipes returns better results than the categories generated with TextRank. More topics need to be added; from looking at the food topic documents, it's clear that the level of granularity with which LDA and NNMF can cluster recipe is very good. Another fix for this issue is to utilize dense word embeddings that capture semantic similarities between words with more sophistication. THe biggest issue with the current model is that the words that maps to each topics or category are limited and discreet. Even if a a words is technically more related to a topic than the words extracted from the same topic, yet the word was not extracted from the topic, then the original word query won't be factored into the search through the categories.\n",
    "\n",
    "Also it does appear that some words are more heavily weighted than others, which biases the search results towards that ingredient, although this does require more rigorous texting. \"Miso\" is a word that is heavily weighted in the tfidf matrices for example. One work around is to use simple rank this ingredient lower in the Search_Recipes function, but a global solution is preferable. It is perhaps more beneficial to utilize these weights that tf-idf creates, rather than finding a way to get rid of them. But experimenting with different word embeddings would be interesting.\n",
    "\n",
    "Also, another issue is that many recipes were not assigned categories due to the model parameters, and this decreases there ranks with the text with an unfair disadvantage. Hopefully a future iteration of this model will allow all recipes to have associated categories.\n",
    "\n",
    "Future Implementation and Changes for this model:\n",
    "\n",
    "- Word2Vec or GloVe embeddings\n",
    "- LDA2Vec topic extraction\n",
    "- Negative Querying that decreases rank of matching queries\n",
    "- Using real databases to store data and creating a creating an user interface on which this model where this model can be easily utilized\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "query = ['jelly','wine']\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['pepper','apple','pork']\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['tags'][122894]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------\n",
    "### Some notes:\n",
    "List of Parameters and Evaluation Methods\n",
    "- Number of Topics\n",
    "- Number of Documents to pull keywords from\n",
    "- Number of Keywords per topic\n",
    "- Number of Documents to assign keywords to\n",
    "- Neighbor Window Size\n",
    "- Query Title Weight\n",
    "- Query Description Weight\n",
    "- Query Category Weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No Category Weight\n",
    "query = ['cream','banana','cinnamon']\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Empty Query\n",
    "query = []\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only Category Weight\n",
    "query = ['apple','blueberry']\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only Category Weight\n",
    "query = ['japanese']\n",
    "Search_Recipes(query, query_ranked=False, recipe_range=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Further Analysis:\n",
    "- Generate Tag Count column in the Recipes data frame. Analyze distribution of tags.\n",
    "- See if all of the topics are easily interpretable from the generated tags."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peerings into the generated topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[122907]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[90708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[50409]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[30234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[23596]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[60457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.tags[110997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOpenAIError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[1;32m----> 2\u001B[0m openai_client \u001B[38;5;241m=\u001B[39m \u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m openai_client\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msk-dbu1G4qqd4cAIE8QENzoT3BlbkFJe7R28eIfCcqxUJF6BgGZ\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m response \u001B[38;5;241m=\u001B[39m openai_client\u001B[38;5;241m.\u001B[39mimages\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m      7\u001B[0m   model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdall-e-3\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      8\u001B[0m   prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma white siamese cat\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m   n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     12\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\recipix\\lib\\site-packages\\openai\\_client.py:98\u001B[0m, in \u001B[0;36mOpenAI.__init__\u001B[1;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001B[0m\n\u001B[0;32m     96\u001B[0m     api_key \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m api_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 98\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m OpenAIError(\n\u001B[0;32m     99\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    100\u001B[0m     )\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m api_key\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m organization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mOpenAIError\u001B[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T15:01:02.074235Z",
     "start_time": "2024-03-30T15:01:02.027231Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:34:47.358169Z",
     "start_time": "2024-03-31T06:34:38.007442Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-AcrPeShL2hm9BoSqtxdSHQjn/user-JMukIn3LRA1xPKTryKczhCT4/img-gJsbu3gGSfPcCASOlBJw4Rux.png?st=2024-03-31T05%3A34%3A47Z&se=2024-03-31T07%3A34%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-03-31T00%3A21%3A32Z&ske=2024-04-01T00%3A21%3A32Z&sks=b&skv=2021-08-06&sig=BEN/CQvNA/s7gMAYkvSmsG1iE%2Brd4rP41JcZSXlI%2Bp4%3D'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:34:55.339680Z",
     "start_time": "2024-03-31T06:34:55.322802Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded and saved as 'downloaded_image.png'.\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T06:37:24.674880Z",
     "start_time": "2024-03-31T06:37:22.283526Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
